{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df3e229a",
      "metadata": {
        "id": "df3e229a"
      },
      "source": [
        "# CS 5542 — Lab 4 Notebook (Team Project)\n",
        "## RAG Application Integration, Deployment, and Monitoring (Deadline: Feb. 12, 2026)\n",
        "\n",
        "**Purpose:** This notebook is a **project-aligned template** for Lab 4. Your team should reuse your Lab-3 multimodal RAG pipeline and integrate it into a **deployable application** with **automatic logging** and **failure analysis**.\n",
        "\n",
        "### Submission policy\n",
        "- **Survey:** submitted **individually**\n",
        "- **Deliverables (GitHub repo / notebook / report / deployment link):** submitted **as a team**\n",
        "\n",
        "### Team-size requirement\n",
        "- **1–2 students:** Base requirements + **1 extension**\n",
        "- **3–4 students:** Base requirements + **2–3 extensions**\n",
        "\n",
        "---\n",
        "\n",
        "## What you will build (at minimum)\n",
        "1. A **Streamlit app** that accepts a question and returns:\n",
        "   - an **answer**\n",
        "   - **retrieved evidence** with citations\n",
        "   - **metrics panel** (latency, P@5, R@10 if applicable)\n",
        "2. An **automatic logger** that appends to: `logs/query_metrics.csv`\n",
        "3. A **mini gold set** of **5 project queries** (Q1–Q5) for evaluation\n",
        "4. **Two failure cases** with root cause + proposed fix\n",
        "\n",
        "> **Important:** Lab 4 focuses on **application integration and deployment**, not on redesigning retrieval. Prefer reusing your Lab-3 modules.\n",
        "\n",
        "---\n",
        "\n",
        "## Recommended repository structure (for your team repo)\n",
        "```\n",
        "/app/              # Streamlit UI (required)\n",
        "/rag/              # Retrieval + indexing modules (reuse from Lab 3)\n",
        "/logs/             # query_metrics.csv (auto-created)\n",
        "/data/             # your project-aligned PDFs/images (do NOT commit large/private data)\n",
        "/api/              # optional FastAPI backend (extension)\n",
        "/notebooks/        # this notebook\n",
        "requirements.txt\n",
        "README.md\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Contents of this notebook\n",
        "1. Setup & environment checks  \n",
        "2. Project dataset wiring (connect your Lab-3 ingestion)  \n",
        "3. Mini gold set (Q1–Q5)  \n",
        "4. Retrieval + answer function (reuse your Lab-3 pipeline)  \n",
        "5. Evaluation + logging (required)  \n",
        "6. Streamlit app skeleton (required)  \n",
        "7. Optional extension: FastAPI backend  \n",
        "8. Deployment checklist + failure analysis template\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC-fL9Z4Q-8g"
      },
      "source": [
        "# Task: Load provided demo files (docs + images)\n",
        "# Place Your_zip_files.zip next to this notebook or upload it to the runtime.\n",
        "import os, zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "ZIP_NAME = 'Your_zip_files.zip'\n",
        "zip_candidates = [\n",
        "    ZIP_NAME,\n",
        "    f'/mnt/data/{ZIP_NAME}',\n",
        "    f'./{ZIP_NAME}',\n",
        "]\n",
        "\n",
        "zip_path = None\n",
        "for p in zip_candidates:\n",
        "    if os.path.exists(p):\n",
        "        zip_path = p\n",
        "        break\n",
        "\n",
        "if zip_path is None:\n",
        "    print('⚠️ Demo ZIP not found. If running in Colab/Kaggle, upload:', ZIP_NAME)\n",
        "else:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall('.')\n",
        "    print('✅ Extracted demo files from:', zip_path)\n",
        "\n",
        "# Expected folder structure after extraction:\n",
        "# ./data/docs/*.txt\n",
        "# ./data/images/*.(png|jpg)\n",
        "print('Docs folder exists:', os.path.isdir('./data/docs'))\n",
        "print('Images folder exists:', os.path.isdir('./data/images'))\n",
        "\n",
        "if os.path.isdir('./data/docs'):\n",
        "    print('Sample docs:', sorted(os.listdir('./data/docs'))[:6])\n",
        "if os.path.isdir('./data/images'):\n",
        "    print('Sample images:', sorted(os.listdir('./data/images'))[:6])\n"
      ],
      "id": "nC-fL9Z4Q-8g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdz6kAJaThoY"
      },
      "source": [
        "# Ensure a numeric/table-like demo file exists for Q4\n",
        "import os\n",
        "numeric_path = './data/docs/07_numeric_table.txt'\n",
        "if not os.path.exists(numeric_path):\n",
        "    with open(numeric_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\n",
        "            'Fusion Hyperparameters (Table 1)\\n'\n",
        "            'alpha = 0.50\\n'\n",
        "            'top_k = 5\\n'\n",
        "            'missing_evidence_score_threshold = 0.05\\n'\n",
        "            'latency_alert_ms = 2000\\n'\n",
        "        )\n",
        "    print('✅ Created:', numeric_path)\n",
        "else:\n",
        "    print('✅ Numeric demo file already present:', numeric_path)\n"
      ],
      "id": "Jdz6kAJaThoY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AOvgEV9Q-8g"
      },
      "source": [
        "# Sanity checks: ensure demo docs are loaded\n",
        "import os, glob\n",
        "doc_files = glob.glob('./data/docs/*.txt')\n",
        "print('Found .txt docs:', len(doc_files))\n",
        "assert len(doc_files) > 0, 'No docs found. Ensure the demo ZIP was extracted and ./data/docs exists.'\n",
        "\n",
        "# Preview one document\n",
        "with open(doc_files[0], 'r', encoding='utf-8') as f:\n",
        "    preview = f.read()[:600]\n",
        "print('Preview:', os.path.basename(doc_files[0]))\n",
        "print(preview)\n"
      ],
      "id": "8AOvgEV9Q-8g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGbb-SjVSN21"
      },
      "source": [
        "# Load demo documents into a single list used throughout the notebook\n",
        "import glob, os\n",
        "\n",
        "DOC_DIR = './data/docs'\n",
        "doc_files = sorted(glob.glob(os.path.join(DOC_DIR, '*.txt')))\n",
        "if len(doc_files) == 0:\n",
        "    raise RuntimeError('No .txt documents found in ./data/docs. Ensure the demo ZIP was extracted.')\n",
        "\n",
        "documents = []\n",
        "for p in doc_files:\n",
        "    with open(p, 'r', encoding='utf-8') as f:\n",
        "        txt = f.read().strip()\n",
        "    if not txt:\n",
        "        continue\n",
        "    doc_id = os.path.basename(p)\n",
        "    documents.append({'doc_id': doc_id, 'source': p, 'text': txt})\n",
        "\n",
        "print('✅ Loaded documents:', len(documents))\n",
        "print('Example doc_id:', documents[0]['doc_id'])\n",
        "print(documents[0]['text'][:300])\n"
      ],
      "id": "xGbb-SjVSN21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLkoKvfRbK41"
      },
      "source": [
        "# Load demo images and create lightweight text surrogates (captions) for multimodal retrieval\n",
        "import glob, os\n",
        "\n",
        "IMG_DIR = './data/images'\n",
        "img_files = sorted(glob.glob(os.path.join(IMG_DIR, '*.*')))\n",
        "img_files = [p for p in img_files if p.lower().endswith(('.png','.jpg','.jpeg','.webp'))]\n",
        "\n",
        "# Minimal captions so images participate in retrieval without requiring a vision encoder\n",
        "IMAGE_CAPTIONS = {\n",
        "    'rag_pipeline.png': 'RAG pipeline diagram: ingest, chunk, index, retrieve top-k evidence, build context, generate grounded answer, log metrics for monitoring.',\n",
        "    'retrieval_modes.png': 'Retrieval modes diagram: BM25 keyword, vector semantic, hybrid fusion, multi-hop hop-1 to hop-2 refinement.',\n",
        "}\n",
        "\n",
        "images = []\n",
        "for p in img_files:\n",
        "    fid = os.path.basename(p)\n",
        "    cap = IMAGE_CAPTIONS.get(fid, fid.replace('_',' ').replace('.png','').replace('.jpg',''))\n",
        "    images.append({'img_id': fid, 'source': p, 'text': cap})\n",
        "\n",
        "print('✅ Loaded images:', len(images))\n",
        "if images:\n",
        "    print('Example image:', images[0]['img_id'])\n",
        "    print('Caption:', images[0]['text'])\n",
        "\n",
        "# Unified evidence store used by retrieval (text + images)\n",
        "items = []\n",
        "for d in documents:\n",
        "    items.append({\n",
        "        'evidence_id': d.get('doc_id') or os.path.basename(d.get('source','')),\n",
        "        'modality': 'text',\n",
        "        'source': d.get('source'),\n",
        "        'text': d.get('text','')\n",
        "    })\n",
        "for im in images:\n",
        "    items.append({\n",
        "        'evidence_id': f\"img::{im['img_id']}\",\n",
        "        'modality': 'image',\n",
        "        'source': im.get('source'),\n",
        "        'text': im.get('text','')\n",
        "    })\n",
        "\n",
        "assert len(items) > 0, 'Evidence store is empty.'\n",
        "print('✅ Unified evidence items:', len(items), '(text:', len(documents), ', images:', len(images), ')')\n"
      ],
      "id": "vLkoKvfRbK41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "47e549d1",
      "metadata": {
        "id": "47e549d1"
      },
      "source": [
        "# 1) Setup & environment checks\n",
        "\n",
        "This notebook includes **safe defaults** and **lightweight code examples**.  \n",
        "Replace the placeholder pieces with your Lab-3 implementation (PDF parsing, OCR, multimodal evidence, hybrid retrieval, reranking).\n",
        "\n",
        "### Install dependencies (edit as needed)\n",
        "- Core: `streamlit`, `pandas`, `numpy`, `requests`\n",
        "- Optional: `fastapi`, `uvicorn` (if you do the FastAPI extension)\n",
        "- Retrieval examples: `scikit-learn` (TF-IDF baseline), optionally `sentence-transformers` (dense embeddings)\n",
        "\n",
        "> In your team repo, always keep a clean `requirements.txt` for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "425991a8",
      "metadata": {
        "id": "425991a8"
      },
      "outputs": [],
      "source": [
        "# If running in Colab or fresh environment, uncomment installs:\n",
        "# !pip -q install streamlit pandas numpy requests scikit-learn\n",
        "# # Optional (FastAPI extension):\n",
        "# !pip -q install fastapi uvicorn pydantic\n",
        "# # Optional (dense retrieval):\n",
        "# !pip -q install sentence-transformers\n",
        "\n",
        "import os, json, time\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Python OK. Working directory:\", os.getcwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c805bf7",
      "metadata": {
        "id": "5c805bf7"
      },
      "source": [
        "# 2) Project paths + configuration\n",
        "\n",
        "Set your project data paths and key parameters here.\n",
        "\n",
        "- Do **not** hardcode secrets (API keys) in notebooks or repos.\n",
        "- If you use a hosted LLM, read from environment variables locally.\n",
        "\n",
        "**Tip:** Keep these settings mirrored in `rag/config.py` so your Streamlit app uses the same config.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d483405",
      "metadata": {
        "id": "1d483405"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Lab4Config:\n",
        "    project_name: str = \"YOUR_PROJECT_NAME\"\n",
        "    data_dir: str = \"./data\"        # where your PDFs/images live locally\n",
        "    logs_dir: str = \"./logs\"\n",
        "    log_file: str = \"./logs/query_metrics.csv\"\n",
        "    top_k_default: int = 10\n",
        "    eval_p_at: int = 5\n",
        "    eval_r_at: int = 10\n",
        "\n",
        "cfg = Lab4Config()\n",
        "Path(cfg.logs_dir).mkdir(parents=True, exist_ok=True)\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c5030e",
      "metadata": {
        "id": "d5c5030e"
      },
      "source": [
        "# 3) Dataset wiring (project-aligned)\n",
        "\n",
        "For Lab 4, your **data, application UI, and models** must be aligned to your team project.\n",
        "\n",
        "## Required (project-aligned)\n",
        "- 2–6 PDFs\n",
        "- 5–15 images/figures/tables (if your project is multimodal)\n",
        "\n",
        "## In Lab 3 you likely had:\n",
        "- PDF text extraction (PyMuPDF)\n",
        "- OCR / captions for figures or scanned pages\n",
        "- Chunking + indexing (dense/sparse/hybrid)\n",
        "- Reranking (optional)\n",
        "- Grounded answer generation with citations\n",
        "\n",
        "### What to do here\n",
        "1. Point this notebook to your dataset folder.\n",
        "2. Load *already-prepared* chunks/evidence from Lab 3 (recommended), OR\n",
        "3. Call your Lab-3 ingestion function to rebuild the index.\n",
        "\n",
        "Below is a **minimal example** that loads plain text files as “documents” so the notebook is runnable even without PDFs.\n",
        "Replace it with your Lab-3 ingestion code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f0f725",
      "metadata": {
        "id": "20f0f725"
      },
      "outputs": [],
      "source": [
        "# Minimal runnable loader (replace with your Lab-3 ingestion + chunking)\n",
        "# Expected structure (example):\n",
        "# ./data/\n",
        "#   docs/\n",
        "#     doc1.txt\n",
        "#     doc2.txt\n",
        "#\n",
        "# For PDFs/images, reuse your Lab-3 ingestion + chunking and store chunks as JSONL/CSV.\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "docs_dir = Path(cfg.data_dir) / \"docs\"\n",
        "docs_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create a tiny demo corpus if empty (so the notebook runs)\n",
        "demo_file = docs_dir / \"demo_doc.txt\"\n",
        "if not any(docs_dir.glob(\"*.txt\")):\n",
        "    demo_file.write_text(\n",
        "        \"This is a demo document for Lab 4. Replace this with your project PDFs and evidence chunks.\\n\"\n",
        "        \"Key idea: retrieval quality drives grounded answers. Provide citations for all claims.\\n\"\n",
        "        \"If missing evidence, return: Not enough evidence in the retrieved context.\\n\",\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "def load_text_docs(docs_path: Path):\n",
        "    items = []\n",
        "    for p in sorted(docs_path.glob(\"*.txt\")):\n",
        "        items.append({\"doc_id\": p.stem, \"source\": str(p), \"text\": p.read_text(errors='ignore')})\n",
        "    return items\n",
        "\n",
        "documents = load_text_docs(docs_dir)\n",
        "print(\"Loaded docs:\", len(documents))\n",
        "documents[0].keys(), documents[0][\"doc_id\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9b108aa",
      "metadata": {
        "id": "d9b108aa"
      },
      "source": [
        "# 4) Mini Gold Set (Q1–Q5) — Required\n",
        "\n",
        "Create **5 project-relevant queries** and define a simple evidence rubric.\n",
        "\n",
        "- **Q1–Q3:** typical project queries (answerable using evidence)\n",
        "- **Q4:** multimodal evidence query (table/figure heavy, OCR/captions should help)\n",
        "- **Q5:** missing-evidence or ambiguous query (must trigger safe behavior)\n",
        "\n",
        "For each query, define:\n",
        "- `gold_evidence_ids`: list of evidence identifiers that are relevant (doc_id/page/fig id)\n",
        "- `answer_criteria`: 1–2 bullets\n",
        "- `citation_format`: how you will cite (e.g., `[Doc1 p3]`, `[fig2]`)\n",
        "\n",
        "This enables **consistent evaluation** and makes logging meaningful.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c131bc70",
      "metadata": {
        "id": "c131bc70"
      },
      "outputs": [],
      "source": [
        "# Task: Populate a mini gold set for monitoring and ablation\n",
        "# Evidence identifiers use doc_ids from the demo corpus (file basenames under ./data/docs).\n",
        "\n",
        "mini_gold = [\n",
        "    {\n",
        "        \"query_id\": \"Q1\",\n",
        "        \"question\": \"What is Retrieval-Augmented Generation (RAG) and what does grounding mean?\",\n",
        "        \"gold_evidence_ids\": [\"01_rag_overview.txt\"],\n",
        "        \"answer_criteria\": [\"Defines RAG\", \"Explains grounding\", \"Includes a citation\"],\n",
        "        \"citation_format\": \"[doc_id]\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q2\",\n",
        "        \"question\": \"If the evidence is insufficient, what should the system say?\",\n",
        "        \"gold_evidence_ids\": [\"05_missing_evidence_policy.txt\"],\n",
        "        \"answer_criteria\": [\"Returns the missing-evidence phrase\", \"Includes a citation\"],\n",
        "        \"citation_format\": \"[doc_id]\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q3\",\n",
        "        \"question\": \"Why would you use hybrid retrieval instead of only BM25 or only vectors?\",\n",
        "        \"gold_evidence_ids\": [\"02_hybrid_retrieval.txt\"],\n",
        "        \"answer_criteria\": [\"Mentions BM25 strengths\", \"Mentions vector strengths\", \"Explains fusion\", \"Includes a citation\"],\n",
        "        \"citation_format\": \"[doc_id]\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q4\",\n",
        "        \"question\": \"From Table 1, what is the value of alpha used for fusion?\",\n",
        "        \"gold_evidence_ids\": [\"07_numeric_table.txt\"],\n",
        "        \"answer_criteria\": [\"Extracts the numeric value 0.50\", \"Includes a citation\"],\n",
        "        \"citation_format\": \"[doc_id]\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q5\",\n",
        "        \"question\": \"Who won the FIFA World Cup in 2050?\",\n",
        "        \"gold_evidence_ids\": [\"N/A\"],\n",
        "        \"answer_criteria\": [\"Returns the missing-evidence phrase\", \"No hallucination\"],\n",
        "        \"citation_format\": \"\"\n",
        "    },\n",
        "]\n",
        "\n",
        "pd.DataFrame(mini_gold)[[\"query_id\",\"question\",\"gold_evidence_ids\"]]\n",
        "\n",
        "\n",
        "\n",
        "# Optional multimodal query (image evidence via caption surrogate)\n",
        "mini_gold.append({\n",
        "    \"query_id\": \"Q6\",\n",
        "    \"question\": \"Which retrieval modes are shown in the retrieval modes diagram?\",\n",
        "    \"gold_evidence_ids\": [\"img::retrieval_modes.png\"],\n",
        "    \"answer_criteria\": [\"Mentions BM25, vector, hybrid, multi-hop\"],\n",
        "    \"citation_format\": \"[evidence_id]\"\n",
        "})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc7GAupjbK43"
      },
      "source": [
        "# Task: Mini gold set (evidence IDs) for evaluation\n",
        "# Evidence IDs refer to demo files under ./data/docs (use basenames). Image evidence uses the prefix img::\n",
        "import pandas as pd\n",
        "\n",
        "mini_gold = [\n",
        "    {\n",
        "        'query_id': 'Q1',\n",
        "        'question': 'What is Retrieval-Augmented Generation (RAG) and what does grounding mean?',\n",
        "        'gold_evidence_ids': ['01_rag_overview.txt']\n",
        "    },\n",
        "    {\n",
        "        'query_id': 'Q2',\n",
        "        'question': 'If the evidence is insufficient, what should the system say?',\n",
        "        'gold_evidence_ids': ['05_missing_evidence_policy.txt']\n",
        "    },\n",
        "    {\n",
        "        'query_id': 'Q3',\n",
        "        'question': 'Why would you use hybrid retrieval instead of only BM25 or only vectors?',\n",
        "        'gold_evidence_ids': ['02_hybrid_retrieval.txt']\n",
        "    },\n",
        "    {\n",
        "        'query_id': 'Q4',\n",
        "        'question': 'From Table 1, what is the value of alpha used for fusion?',\n",
        "        'gold_evidence_ids': ['07_numeric_table.txt']\n",
        "    },\n",
        "    {\n",
        "        'query_id': 'Q5',\n",
        "        'question': 'Who won the FIFA World Cup in 2050?',\n",
        "        'gold_evidence_ids': ['N/A']\n",
        "    },\n",
        "    {\n",
        "        'query_id': 'Q6',\n",
        "        'question': 'Which retrieval modes are shown in the retrieval modes diagram?',\n",
        "        'gold_evidence_ids': ['img::retrieval_modes.png']\n",
        "    },\n",
        "]\n",
        "\n",
        "pd.DataFrame(mini_gold)[['query_id','question','gold_evidence_ids']]\n"
      ],
      "id": "Rc7GAupjbK43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6da3e87e",
      "metadata": {
        "id": "6da3e87e"
      },
      "source": [
        "# 5) Retrieval + Answer Function (Reuse Lab 3)\n",
        "\n",
        "Below is a **baseline TF‑IDF retriever** so this notebook is runnable.\n",
        "Replace with your Lab-3 retrieval stack:\n",
        "- dense (SentenceTransformers + FAISS/Chroma)\n",
        "- sparse (BM25)\n",
        "- hybrid fusion\n",
        "- optional reranking\n",
        "\n",
        "### Required output contract (recommended)\n",
        "Your retrieval function should return a list of evidence items:\n",
        "- `chunk_id` or `doc_id`\n",
        "- `source`\n",
        "- `score`\n",
        "- `citation_tag` (e.g., `[Doc1 p3]`, `[fig2]`)\n",
        "- `text` (the evidence text shown to users)\n",
        "\n",
        "Your answer function must enforce:\n",
        "- **Citations for claims**\n",
        "- If missing evidence: **return exactly**  \n",
        "  `Not enough evidence in the retrieved context.`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f60c59",
      "metadata": {
        "id": "e1f60c59"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Build a simple TF-IDF index over documents (demo baseline)\n",
        "corpus = [d[\"text\"] for d in documents]\n",
        "doc_ids = [d[\"doc_id\"] for d in documents]\n",
        "sources = [d[\"source\"] for d in documents]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "def retrieve_tfidf(question: str, top_k: int = 5):\n",
        "    q = vectorizer.transform([question])\n",
        "    sims = cosine_similarity(q, X).ravel()\n",
        "    idxs = np.argsort(-sims)[:top_k]\n",
        "    evidence = []\n",
        "    for rank, i in enumerate(idxs):\n",
        "        evidence.append({\n",
        "            \"chunk_id\": doc_ids[i],\n",
        "            \"source\": sources[i],\n",
        "            \"score\": float(sims[i]),\n",
        "            \"citation_tag\": f\"[{doc_ids[i]}]\",\n",
        "            \"text\": corpus[i][:800]  # truncate for UI\n",
        "        })\n",
        "    return evidence\n",
        "\n",
        "MISSING_EVIDENCE_MSG = \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "def generate_answer_stub(question: str, evidence: list):\n",
        "    \"\"\"Replace with your LLM/VLM generation.\n",
        "    For this template we produce a simple grounded response.\n",
        "    \"\"\"\n",
        "    if not evidence or max(e.get(\"score\", 0.0) for e in evidence) < 0.05:\n",
        "        return MISSING_EVIDENCE_MSG\n",
        "\n",
        "    # Minimal grounded \"answer\" example: summarize top evidence\n",
        "    top = evidence[0]\n",
        "    answer = (\n",
        "        f\"Based on the retrieved evidence {top['citation_tag']}, \"\n",
        "        f\"the system should ground its response in retrieved context and cite sources. \"\n",
        "        f\"If evidence is missing, it must respond with: '{MISSING_EVIDENCE_MSG}'. \"\n",
        "        f\"{top['citation_tag']}\"\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "# Quick test\n",
        "test_q = mini_gold[0][\"question\"]\n",
        "ev = retrieve_tfidf(test_q, top_k=3)\n",
        "print(\"Top evidence:\", ev[0][\"chunk_id\"], ev[0][\"score\"])\n",
        "print(\"Answer:\", generate_answer_stub(test_q, ev))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7629d00",
      "metadata": {
        "id": "d7629d00"
      },
      "source": [
        "# 6) Evaluation + Logging (Required)\n",
        "\n",
        "Every query must append to: `logs/query_metrics.csv`\n",
        "\n",
        "Required columns (minimum):\n",
        "- timestamp\n",
        "- query_id\n",
        "- retrieval_mode\n",
        "- top_k\n",
        "- latency_ms\n",
        "- Precision@5\n",
        "- Recall@10\n",
        "- evidence_ids_returned\n",
        "- faithfulness_pass\n",
        "- missing_evidence_behavior\n",
        "\n",
        "> If your gold set is incomplete (common for Q4/Q5), compute P/R only for labeled queries and still log latency/evidence IDs.\n",
        "\n",
        "## How we define metrics (simple)\n",
        "- `Precision@K`: (# retrieved evidence IDs in gold) / K\n",
        "- `Recall@K`: (# retrieved evidence IDs in gold) / (size of gold set)\n",
        "\n",
        "**Faithfulness (Yes/No):**\n",
        "- Yes if the answer **only** uses retrieved evidence and includes citations.\n",
        "- For this template, we implement a simple heuristic. Replace with your rubric/judge if desired.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "850487f0",
      "metadata": {
        "id": "850487f0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "\n",
        "\n",
        "def _canon_evidence_id(x: str) -> str:\n",
        "    x = str(x).strip()\n",
        "    # keep img:: prefix intact\n",
        "    if x.startswith('img::'):\n",
        "        return x\n",
        "    # normalize file ids: allow with/without extension\n",
        "    if x.endswith('.txt'):\n",
        "        return x[:-4]\n",
        "    return x\n",
        "\n",
        "def _normalize_retrieved_ids(retrieved):\n",
        "    \"\"\"Normalize retrieved outputs into a list of evidence IDs.\n",
        "    Returns canonical IDs (doc_id without .txt, or img::filename).\n",
        "\n",
        "    Supports: list[dict], list[(idx,score)], list[str].\n",
        "    \"\"\"\n",
        "    if retrieved is None:\n",
        "        return []\n",
        "    if len(retrieved) == 0:\n",
        "        return []\n",
        "    # list[str]\n",
        "    if isinstance(retrieved[0], str):\n",
        "        return [_canon_evidence_id(r) for r in retrieved]\n",
        "    # list[dict]\n",
        "    if isinstance(retrieved[0], dict):\n",
        "        out=[]\n",
        "        for r in retrieved:\n",
        "            if 'evidence_id' in r and r['evidence_id']:\n",
        "                out.append(_canon_evidence_id(r['evidence_id']))\n",
        "            elif 'doc_id' in r and r['doc_id']:\n",
        "                out.append(_canon_evidence_id(r['doc_id']))\n",
        "            elif 'source' in r and r['source']:\n",
        "                out.append(_canon_evidence_id(os.path.basename(str(r['source']))))\n",
        "        return out\n",
        "    # list[(idx, score)]\n",
        "    if isinstance(retrieved[0], (tuple, list)) and len(retrieved[0]) >= 1:\n",
        "        out=[]\n",
        "        for item in retrieved:\n",
        "            idx = int(item[0])\n",
        "            if 'items' in globals() and 0 <= idx < len(items):\n",
        "                out.append(_canon_evidence_id(items[idx].get('evidence_id')))\n",
        "            elif 'documents' in globals() and 0 <= idx < len(documents):\n",
        "                out.append(_canon_evidence_id(documents[idx].get('doc_id') or os.path.basename(documents[idx].get('source',''))))\n",
        "        return out\n",
        "    return []\n",
        "\n",
        "def _normalize_gold_ids(gold_ids):\n",
        "    if not gold_ids or gold_ids == ['N/A']:\n",
        "        return None\n",
        "    return [_canon_evidence_id(g) for g in gold_ids]\n",
        "\n",
        "def precision_at_k(retrieved, gold_ids, k):\n",
        "    gold = _normalize_gold_ids(gold_ids)\n",
        "    if gold is None:\n",
        "        return None\n",
        "    retrieved_ids = _normalize_retrieved_ids(retrieved)[:k]\n",
        "    if k == 0:\n",
        "        return None\n",
        "    return len(set(retrieved_ids) & set(gold)) / float(k)\n",
        "\n",
        "def recall_at_k(retrieved, gold_ids, k):\n",
        "    gold = _normalize_gold_ids(gold_ids)\n",
        "    if gold is None:\n",
        "        return None\n",
        "    retrieved_ids = _normalize_retrieved_ids(retrieved)[:k]\n",
        "    denom = float(len(set(gold)))\n",
        "    return (len(set(retrieved_ids) & set(gold)) / denom) if denom > 0 else None\n",
        "\n",
        "\n",
        "\n",
        "def faithfulness_heuristic(answer: str, evidence: list):\n",
        "    # Simple heuristic: answer includes at least one citation tag from evidence OR is missing-evidence msg\n",
        "    if answer.strip() == MISSING_EVIDENCE_MSG:\n",
        "        return True\n",
        "    tags = [e[\"citation_tag\"] for e in evidence[:5]]\n",
        "    return any(tag in answer for tag in tags)\n",
        "\n",
        "def missing_evidence_behavior(answer: str, evidence: list):\n",
        "    # Pass if either: evidence present and answer not missing-evidence; or evidence absent and answer is missing-evidence msg\n",
        "    has_ev = bool(evidence) and max(e.get(\"score\", 0.0) for e in evidence) >= 0.05\n",
        "    if not has_ev:\n",
        "        return \"Pass\" if answer.strip() == MISSING_EVIDENCE_MSG else \"Fail\"\n",
        "    else:\n",
        "        return \"Pass\" if answer.strip() != MISSING_EVIDENCE_MSG else \"Fail\"\n",
        "\n",
        "def ensure_logfile(path: str, header: list):\n",
        "    p = Path(path)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if not p.exists():\n",
        "        with open(p, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(header)\n",
        "\n",
        "LOG_HEADER = [\n",
        "    \"timestamp\", \"query_id\", \"retrieval_mode\", \"top_k\", \"latency_ms\",\n",
        "    \"Precision@5\", \"Recall@10\",\n",
        "    \"evidence_ids_returned\", \"gold_evidence_ids\",\n",
        "    \"faithfulness_pass\", \"missing_evidence_behavior\"\n",
        "]\n",
        "ensure_logfile(cfg.log_file, LOG_HEADER)\n",
        "\n",
        "def run_query_and_log(query_item, retrieval_mode = 'hybrid', top_k=10):\n",
        "    question = query_item[\"question\"]\n",
        "    gold_ids = query_item.get(\"gold_evidence_ids\", [])\n",
        "\n",
        "    t0 = time.time()\n",
        "    evidence = retrieve_tfidf(question, top_k=top_k)  # replace with your pipeline + modes\n",
        "    answer = generate_answer_stub(question, evidence) # replace with LLM/VLM\n",
        "    latency_ms = (time.time() - t0) * 1000.0\n",
        "\n",
        "    retrieved_ids = [e[\"chunk_id\"] for e in evidence]\n",
        "    p5 = precision_at_k(retrieved_ids, gold_ids, cfg.eval_p_at) if gold_ids else np.nan\n",
        "    r10 = recall_at_k(retrieved_ids, gold_ids, cfg.eval_r_at) if gold_ids else np.nan\n",
        "\n",
        "    faithful = faithfulness_heuristic(answer, evidence)\n",
        "    meb = missing_evidence_behavior(answer, evidence)\n",
        "\n",
        "    row = [\n",
        "        datetime.now(timezone.utc).isoformat(),\n",
        "        query_item[\"query_id\"],\n",
        "        retrieval_mode,\n",
        "        top_k,\n",
        "        round(latency_ms, 2),\n",
        "        p5,\n",
        "        r10,\n",
        "        json.dumps(retrieved_ids),\n",
        "        json.dumps(gold_ids),\n",
        "        \"Yes\" if faithful else \"No\",\n",
        "        meb\n",
        "    ]\n",
        "    with open(cfg.log_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(row)\n",
        "\n",
        "    return {\"answer\": answer, \"evidence\": evidence, \"p5\": p5, \"r10\": r10, \"latency_ms\": latency_ms, \"faithful\": faithful, \"meb\": meb}\n",
        "\n",
        "# Run all five queries once (demo)\n",
        "results = []\n",
        "for qi in mini_gold:\n",
        "    results.append(run_query_and_log(qi, retrieval_mode = 'hybrid', top_k=cfg.top_k_default))\n",
        "\n",
        "pd.read_csv(cfg.log_file).tail(8)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ungUxBVhbK44"
      },
      "source": [
        "# Task: Run retrieval + answer generation for all mini-gold queries\n",
        "# This cell is self-contained: if retrieval/indexing cells were skipped, it will bootstrap a TF-IDF retriever.\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Build a local evidence list if not already present\n",
        "if 'items' in globals():\n",
        "    _evidence = items\n",
        "elif 'documents' in globals():\n",
        "    _evidence = []\n",
        "    for d in documents:\n",
        "        _evidence.append({\n",
        "            'evidence_id': d.get('doc_id') or os.path.basename(d.get('source','')),\n",
        "            'modality': 'text',\n",
        "            'source': d.get('source'),\n",
        "            'text': d.get('text','')\n",
        "        })\n",
        "else:\n",
        "    raise NameError('Neither items nor documents are defined. Run the ZIP extraction + document loading cells first.')\n",
        "\n",
        "assert len(_evidence) > 0, 'Evidence store is empty.'\n",
        "\n",
        "# Canonicalize evidence ids for consistent evaluation\n",
        "def _canon_evidence_id(x: str) -> str:\n",
        "    x = str(x).strip()\n",
        "    if x.startswith('img::'):\n",
        "        return x\n",
        "    return x[:-4] if x.endswith('.txt') else x\n",
        "\n",
        "# Bootstrap TF-IDF retriever if no retriever exists\n",
        "if 'retrieve_hybrid' not in globals() and 'retrieve_tfidf' not in globals() and 'retrieve' not in globals():\n",
        "    _texts = [it.get('text','') for it in _evidence]\n",
        "    _tfidf = TfidfVectorizer(stop_words=None, token_pattern=r'(?u)\\b\\w+\\b')\n",
        "    _tfidf_mat = _tfidf.fit_transform(_texts)\n",
        "\n",
        "    def retrieve_tfidf(query, top_k=10):\n",
        "        qv = _tfidf.transform([query])\n",
        "        sims = cosine_similarity(qv, _tfidf_mat).ravel()\n",
        "        idx = np.argsort(sims)[::-1][:top_k]\n",
        "        return [(int(i), float(sims[i])) for i in idx]\n",
        "\n",
        "# Define retrieve() wrapper if missing\n",
        "if 'retrieve' not in globals():\n",
        "    def retrieve(question, retrieval_mode='hybrid', top_k=10, alpha=0.6):\n",
        "        # Prefer hybrid if available; otherwise TF-IDF\n",
        "        if retrieval_mode == 'hybrid' and 'retrieve_hybrid' in globals():\n",
        "            hits = retrieve_hybrid(question, top_k=top_k, alpha=alpha)\n",
        "            return hits, {'mode':'hybrid'}\n",
        "        if 'retrieve_tfidf' in globals():\n",
        "            hits = retrieve_tfidf(question, top_k=top_k)\n",
        "            return hits, {'mode':'tfidf'}\n",
        "        raise NameError('No retriever available. Execute the retrieval/indexing section.')\n",
        "\n",
        "# Ensure build_context exists\n",
        "if 'build_context' not in globals():\n",
        "    def build_context(hit_ids, max_chars=1400):\n",
        "        parts=[]\n",
        "        for i in hit_ids:\n",
        "            parts.append(f\"[{_evidence[i].get('evidence_id')}] {_evidence[i].get('text','')}\")\n",
        "        ctx='\\n'.join(parts)\n",
        "        return ctx[:max_chars]\n",
        "\n",
        "# Ensure extractive_answer exists\n",
        "if 'extractive_answer' not in globals():\n",
        "    import re\n",
        "    def extractive_answer(query, context):\n",
        "        q=set(re.findall(r'[A-Za-z]+', query.lower()))\n",
        "        sents=re.split(r'(?<=[.!?])\\s+', (context or '').strip())\n",
        "        scored=[]\n",
        "        for s in sents:\n",
        "            w=set(re.findall(r'[A-Za-z]+', s.lower()))\n",
        "            scored.append((len(q & w), s.strip()))\n",
        "        scored.sort(key=lambda x:x[0], reverse=True)\n",
        "        best=[s for sc,s in scored[:3] if sc>0]\n",
        "        return ' '.join(best) if best else 'Not enough information in the context.'\n",
        "\n",
        "rows=[]\n",
        "for ex in mini_gold:\n",
        "    qid = ex.get('query_id')\n",
        "    question = ex.get('question')\n",
        "    gold = ex.get('gold_evidence_ids')\n",
        "\n",
        "    if 'run_query_and_log' in globals():\n",
        "        # Call run_query_and_log with the full query item dictionary 'ex'\n",
        "        out = run_query_and_log(ex, retrieval_mode='hybrid', top_k=10)\n",
        "        answer = out.get('answer')\n",
        "        # The 'evidence' key from run_query_and_log output contains a list of dicts with 'chunk_id'\n",
        "        evidence = [e['chunk_id'] for e in out.get('evidence', [])]\n",
        "    else:\n",
        "        hits, debug = retrieve(question, retrieval_mode='hybrid', top_k=10)\n",
        "        hit_ids = [int(i) for i,_ in hits]\n",
        "        context = build_context(hit_ids[:10])\n",
        "        answer = extractive_answer(question, context)\n",
        "        evidence = [_canon_evidence_id(_evidence[i].get('evidence_id')) for i in hit_ids[:10]]\n",
        "\n",
        "    rows.append({\n",
        "        'query_id': qid,\n",
        "        'question': question,\n",
        "        'answer': answer,\n",
        "        'evidence_ids_returned(top10)': evidence,\n",
        "        'gold_evidence_ids': gold,\n",
        "    })\n",
        "\n",
        "df_answers = pd.DataFrame(rows)\n",
        "df_answers\n"
      ],
      "id": "ungUxBVhbK44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "46e71b22",
      "metadata": {
        "id": "46e71b22"
      },
      "source": [
        "# 7) Streamlit App Skeleton (Required)\n",
        "\n",
        "You will create a Streamlit app file in your repo, e.g.:\n",
        "\n",
        "- `app/main.py`\n",
        "\n",
        "This notebook can generate a starter `app/main.py` for your team.\n",
        "\n",
        "### Required UI components\n",
        "- Query input box\n",
        "- Retrieval controls (mode, top_k, multimodal toggle if applicable)\n",
        "- Answer panel\n",
        "- Evidence panel (with citations)\n",
        "- Metrics panel (latency, P@5, R@10 if available)\n",
        "- Logging happens automatically on each query\n",
        "\n",
        "> This skeleton calls functions in your Python modules. Prefer moving retrieval logic into `/rag/` and importing it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb966c49",
      "metadata": {
        "id": "bb966c49"
      },
      "outputs": [],
      "source": [
        "# Generate a starter Streamlit app file (edit paths as needed).\n",
        "# In your repo: create /app/main.py and move shared logic into /rag/\n",
        "\n",
        "streamlit_code = r'''\n",
        "import json, time\n",
        "from pathlib import Path\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "# --- Import your team pipeline here ---\n",
        "# from rag.pipeline import retrieve, generate_answer, run_query_and_log\n",
        "\n",
        "MISSING_EVIDENCE_MSG = \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "st.set_page_config(page_title=\"CS5542 Lab 4 — Project RAG App\", layout=\"wide\")\n",
        "st.title(\"CS 5542 Lab 4 — Project RAG Application\")\n",
        "st.caption(\"Project-aligned Streamlit UI + automatic logging + failure monitoring\")\n",
        "\n",
        "# Sidebar controls\n",
        "st.sidebar.header(\"Retrieval Settings\")\n",
        "retrieval_mode = st.sidebar.selectbox(\"retrieval_mode\", [\"tfidf\", \"dense\", \"sparse\", \"hybrid\", \"hybrid_rerank\"])\n",
        "top_k = st.sidebar.slider(\"top_k\", min_value=1, max_value=30, value=10, step=1)\n",
        "use_multimodal = st.sidebar.checkbox(\"use_multimodal\", value=True)\n",
        "\n",
        "st.sidebar.header(\"Logging\")\n",
        "log_path = st.sidebar.text_input(\"log file\", value=\"logs/query_metrics.csv\")\n",
        "\n",
        "# --- Mini gold set (replace with your team's Q1–Q5) ---\n",
        "# Tip: keep the same structure as in your Lab 4 notebook so IDs match logs.\n",
        "MINI_GOLD = {\n",
        "    \"Q1\": {\"question\": \"Replace with your project Q1\", \"gold_evidence_ids\": []},\n",
        "    \"Q2\": {\"question\": \"Replace with your project Q2\", \"gold_evidence_ids\": []},\n",
        "    \"Q3\": {\"question\": \"Replace with your project Q3\", \"gold_evidence_ids\": []},\n",
        "    \"Q4\": {\"question\": \"Replace with your project Q4 (multimodal/table/figure)\", \"gold_evidence_ids\": []},\n",
        "    \"Q5\": {\"question\": \"Replace with your project Q5 (missing-evidence case)\", \"gold_evidence_ids\": []},\n",
        "}\n",
        "\n",
        "st.sidebar.header(\"Evaluation\")\n",
        "query_id = st.sidebar.selectbox(\"query_id (for logging)\", list(MINI_GOLD.keys()))\n",
        "use_gold_question = st.sidebar.checkbox(\"Use the gold-set question text\", value=True)\n",
        "\n",
        "# Main query\n",
        "default_q = MINI_GOLD[query_id][\"question\"] if use_gold_question else \"\"\n",
        "question = st.text_area(\"Enter your question\", value=default_q, height=120)\n",
        "run_btn = st.button(\"Run Query\")\n",
        "\n",
        "colA, colB = st.columns([2, 1])\n",
        "\n",
        "def ensure_logfile(path: str):\n",
        "    p = Path(path)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if not p.exists():\n",
        "        df = pd.DataFrame(columns=[\n",
        "            \"timestamp\",\"query_id\",\"retrieval_mode\",\"top_k\",\"latency_ms\",\n",
        "            \"Precision@5\",\"Recall@10\",\"evidence_ids_returned\",\"gold_evidence_ids\",\n",
        "            \"faithfulness_pass\",\"missing_evidence_behavior\"\n",
        "        ])\n",
        "        df.to_csv(p, index=False)\n",
        "\n",
        "def precision_at_k(retrieved_ids, gold_ids, k=5):\n",
        "    if not gold_ids:\n",
        "        return None\n",
        "    topk = retrieved_ids[:k]\n",
        "    hits = sum(1 for x in topk if x in set(gold_ids))\n",
        "    return hits / k\n",
        "\n",
        "def recall_at_k(retrieved_ids, gold_ids, k=10):\n",
        "    if not gold_ids:\n",
        "        return None\n",
        "    topk = retrieved_ids[:k]\n",
        "    hits = sum(1 for x in topk if x in set(gold_ids))\n",
        "    return hits / max(1, len(gold_ids))\n",
        "\n",
        "# ---- Placeholder demo logic (replace with imports from your /rag module) ----\n",
        "def retrieve_demo(q: str, top_k: int):\n",
        "    return [{\"chunk_id\":\"demo_doc\",\"citation_tag\":\"[demo_doc]\",\"score\":0.9,\"source\":\"data/docs/demo_doc.txt\",\"text\":\"demo evidence...\"}]\n",
        "\n",
        "def answer_demo(q: str, evidence: list):\n",
        "    if not evidence:\n",
        "        return MISSING_EVIDENCE_MSG\n",
        "    return f\"Grounded answer using {evidence[0]['citation_tag']} {evidence[0]['citation_tag']}\"\n",
        "\n",
        "def log_row(path: str, row: dict):\n",
        "    ensure_logfile(path)\n",
        "    df = pd.read_csv(path)\n",
        "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "    df.to_csv(path, index=False)\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "if run_btn and question.strip():\n",
        "    t0 = time.time()\n",
        "    evidence = retrieve_demo(question, top_k=top_k)\n",
        "    answer = answer_demo(question, evidence)\n",
        "    latency_ms = round((time.time() - t0)*1000, 2)\n",
        "\n",
        "    retrieved_ids = [e[\"chunk_id\"] for e in evidence]\n",
        "    gold_ids = MINI_GOLD[query_id].get(\"gold_evidence_ids\", [])\n",
        "\n",
        "    p5 = precision_at_k(retrieved_ids, gold_ids, k=5)\n",
        "    r10 = recall_at_k(retrieved_ids, gold_ids, k=10)\n",
        "\n",
        "    with colA:\n",
        "        st.subheader(\"Answer\")\n",
        "        st.write(answer)\n",
        "\n",
        "        st.subheader(\"Evidence (Top-K)\")\n",
        "        st.json(evidence)\n",
        "\n",
        "    with colB:\n",
        "        st.subheader(\"Metrics\")\n",
        "        st.write({\"latency_ms\": latency_ms, \"Precision@5\": p5, \"Recall@10\": r10})\n",
        "\n",
        "    # Log the query using the selected Q1–Q5 ID (not ad-hoc)\n",
        "    row = {\n",
        "        \"timestamp\": pd.Timestamp.utcnow().isoformat(),\n",
        "        \"query_id\": query_id,\n",
        "        \"retrieval_mode\": retrieval_mode,\n",
        "        \"top_k\": top_k,\n",
        "        \"latency_ms\": latency_ms,\n",
        "        \"Precision@5\": p5,\n",
        "        \"Recall@10\": r10,\n",
        "        \"evidence_ids_returned\": json.dumps(retrieved_ids),\n",
        "        \"gold_evidence_ids\": json.dumps(gold_ids),\n",
        "        \"faithfulness_pass\": \"Yes\" if answer != MISSING_EVIDENCE_MSG else \"Yes\",\n",
        "        \"missing_evidence_behavior\": \"Pass\"  # update with your rule if needed\n",
        "    }\n",
        "    log_row(log_path, row)\n",
        "    st.success(f\"Logged {query_id} to CSV.\")\n",
        "'''\n",
        "app_dir = Path(\"app\")\n",
        "app_dir.mkdir(parents=True, exist_ok=True)\n",
        "(app_dir / \"main.py\").write_text(streamlit_code, encoding=\"utf-8\")\n",
        "print(\"Wrote starter Streamlit app to:\", app_dir / \"main.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05c21ec8",
      "metadata": {
        "id": "05c21ec8"
      },
      "source": [
        "# 8) Optional Extension — FastAPI Backend (Recommended for larger teams)\n",
        "\n",
        "If your team selects the **FastAPI extension**, create:\n",
        "- `api/server.py` with `POST /query`\n",
        "- Streamlit UI calls the API using `requests.post(...)`\n",
        "\n",
        "This separation mirrors real production systems:\n",
        "UI (Streamlit) → API (FastAPI) → Retrieval + LLM services\n",
        "\n",
        "Below is a minimal FastAPI starter you can generate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32168ff2",
      "metadata": {
        "id": "32168ff2"
      },
      "outputs": [],
      "source": [
        "fastapi_code = r'''\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "app = FastAPI(title=\"CS5542 Lab 4 RAG Backend\")\n",
        "\n",
        "MISSING_EVIDENCE_MSG = \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "class QueryIn(BaseModel):\n",
        "    question: str\n",
        "    top_k: int = 10\n",
        "    retrieval_mode: str = \"hybrid\"\n",
        "    use_multimodal: bool = True\n",
        "\n",
        "@app.post(\"/query\")\n",
        "def query(q: QueryIn) -> Dict[str, Any]:\n",
        "    # TODO: import your real pipeline:\n",
        "    # evidence = retrieve(q.question, top_k=q.top_k, mode=q.retrieval_mode, use_multimodal=q.use_multimodal)\n",
        "    # answer = generate_answer(q.question, evidence)\n",
        "    evidence = [{\"chunk_id\":\"demo_doc\",\"citation_tag\":\"[demo_doc]\",\"score\":0.9,\"source\":\"data/docs/demo_doc.txt\",\"text\":\"demo evidence...\"}]\n",
        "    answer = f\"Grounded answer using {evidence[0]['citation_tag']} {evidence[0]['citation_tag']}\"\n",
        "    return {\n",
        "        \"answer\": answer,\n",
        "        \"evidence\": evidence,\n",
        "        \"metrics\": {\"top_k\": q.top_k, \"retrieval_mode\": q.retrieval_mode},\n",
        "        \"failure_flag\": False\n",
        "    }\n",
        "'''\n",
        "api_dir = Path(\"api\")\n",
        "api_dir.mkdir(parents=True, exist_ok=True)\n",
        "(api_dir / \"server.py\").write_text(fastapi_code, encoding=\"utf-8\")\n",
        "print(\"Wrote starter FastAPI server to:\", api_dir / \"server.py\")\n",
        "\n",
        "print(\"\\nRun locally (terminal):\")\n",
        "print(\"  uvicorn api.server:app --reload --port 8000\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9351032a",
      "metadata": {
        "id": "9351032a"
      },
      "source": [
        "# 9) Deployment checklist (Required)\n",
        "\n",
        "Choose **one** deployment route and publish the public link in your README:\n",
        "\n",
        "- HuggingFace Spaces (Streamlit)\n",
        "- Streamlit Cloud (GitHub-connected)\n",
        "- Render / Railway (GitHub-connected)\n",
        "\n",
        "## README must include\n",
        "1. Public deployment link  \n",
        "2. How to run locally:\n",
        "   - `pip install -r requirements.txt`\n",
        "   - `streamlit run app/main.py`\n",
        "3. A screenshot of:\n",
        "   - the UI\n",
        "   - evidence panel\n",
        "   - metrics panel\n",
        "4. Results snapshot:\n",
        "   - **5 queries × 2 retrieval modes**\n",
        "5. Failure analysis:\n",
        "   - 2 failure cases, root cause, proposed fix\n",
        "\n",
        "---\n",
        "\n",
        "# 10) Failure analysis template (Required)\n",
        "\n",
        "Document:\n",
        "1. **Retrieval failure** (wrong evidence or missed gold evidence)  \n",
        "2. **Grounding / missing-evidence failure** (safe behavior or citation enforcement)\n",
        "\n",
        "For each:\n",
        "- What happened?\n",
        "- Why did it happen (root cause)?\n",
        "- What change will you implement next?\n",
        "\n",
        "You can paste your analysis into your README under **Lab 4 Results**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67c11a7f",
      "metadata": {
        "id": "67c11a7f"
      },
      "source": [
        "# 11) Team checklist (quick)\n",
        "\n",
        "Before submission, verify:\n",
        "\n",
        "- [ ] Dataset, UI, and models are **project-aligned**\n",
        "- [ ] Streamlit app runs locally and shows: answer + evidence + metrics\n",
        "- [ ] `logs/query_metrics.csv` is auto-created and appended per query\n",
        "- [ ] Mini gold set Q1–Q5 exists and P@5/R@10 computed when possible\n",
        "- [ ] Deployed link is public and listed in README\n",
        "- [ ] Two failure cases documented with fixes\n",
        "- [ ] `requirements.txt` and run instructions are correct\n",
        "- [ ] Individual survey submitted by each teammate\n",
        "\n",
        "---\n",
        "\n",
        "If you want to go beyond: add an evaluation dashboard, reranking integration, or FastAPI separation (extensions).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l9AWSmiSN26"
      },
      "source": [
        "# Verification: retrieval should return non-empty results for the demo queries\n",
        "test_q = \"What is Retrieval-Augmented Generation (RAG) and what does grounding mean?\"\n",
        "\n",
        "try:\n",
        "    # Try common retrieval function names in the template\n",
        "    if 'retrieve_tfidf' in globals():\n",
        "        hits = retrieve_tfidf(test_q, top_k=5)\n",
        "    elif 'retrieve' in globals():\n",
        "        hits = retrieve(test_q, top_k=5)\n",
        "    else:\n",
        "        hits = []\n",
        "    # Normalize hits to a list\n",
        "    if hits is None:\n",
        "        hits = []\n",
        "    # Some functions return list of dicts; some return (idx,score)\n",
        "    n = len(hits) if hasattr(hits, '__len__') else 0\n",
        "    print('Demo retrieval hits:', n)\n",
        "    assert n > 0, 'Retrieval returned empty results. Check indexing cell and corpus construction.'\n",
        "except Exception as e:\n",
        "    print('⚠️ Retrieval verification could not run (template function names differ).')\n",
        "    print('Reason:', type(e).__name__, str(e)[:180])\n"
      ],
      "id": "4l9AWSmiSN26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "71062ba0",
      "metadata": {
        "id": "71062ba0"
      },
      "source": [
        "\n",
        "## GitHub Deployment Example\n",
        "\n",
        "### Step 1 — Push to GitHub\n",
        "```bash\n",
        "git init\n",
        "git add .\n",
        "git commit -m \"Lab4 deployment\"\n",
        "git branch -M main\n",
        "git remote add origin https://github.com/<username>/<repo>.git\n",
        "git push -u origin main\n",
        "```\n",
        "\n",
        "### Step 2 — Deploy using Streamlit Cloud\n",
        "1. Visit https://share.streamlit.io\n",
        "2. Click **New App**\n",
        "3. Select your GitHub repository\n",
        "4. Branch: `main`\n",
        "5. App path: `app/main.py`\n",
        "6. Click **Deploy**\n",
        "\n",
        "### Step 3 — Add deployment link\n",
        "Include the deployed application URL in your README.md file.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}